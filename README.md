Mini Shakespeare Language Model (LLM/SLM)

This project fine-tunes a small GPT-2 model on the Tiny Shakespeare dataset using Hugging Face Transformers. The model learns to generate Shakespeare-style text and demonstrates practical skills in LLM/SLM training, tokenization, and experiment tracking (W&B).

ðŸ”‘ Highlights

Dataset: Tiny Shakespeare (Hugging Face)

Model: GPT-2 (small)

Training tracked with Weights & Biases

Generates realistic Shakespeare-like dialogue

ðŸ“Š Example Output
ROMEO: What light through yonder window breaks?
JULIET: The stars sing softly, yet I cannot rest.

ðŸ“Œ Skills Gained

Language model fine-tuning

Data preprocessing & tokenization

Experiment logging (W&B)

Text generation with Transformers
